"""
PSP outputs (when return_raw_dataframes=True):
- positions_lf: positions with factor/weight columns
- lookthroughs_lf: lookthrough underlyings with record_type, parent_instrument_id
- scale_factors_lf: scale factors per (config, pid, container)

Weight logic by granularity:
- "no": PI = initial_weight, PR = resulting_weight
- lookthrough (has underlying): PI = initial_exposure_weight_{suffix} × weight_{suffix}
- lookthrough (leaf, no underlying): PI = initial_exposure_weight_{suffix} alone
"""

from __future__ import annotations

from typing import Dict, List, Optional

import polars as pl
import polars.selectors as cs
import pytest

from perspective_service.core.engine import PerspectiveEngine


# =============================================================================
# Core Function: calculate_single_lookthrough
# Matches old PositionEngine.calculate_single_lookthrough() exactly
# =============================================================================


def calculate_single_lookthrough(
    positions_lf: pl.LazyFrame,
    lookthroughs_lf: Optional[pl.LazyFrame],
    scale_factors_lf: Optional[pl.LazyFrame],
    config: str,
    pid: int,
    granularity: str,  # "no", "essential_lookthroughs", "complete_lookthroughs"
    value_metadata: Optional[pl.LazyFrame] = None,
    scale_holdings: bool = False,
    additional_attributes: List[str] = None,
) -> pl.LazyFrame:
    """
    Replicate old PositionEngine.calculate_single_lookthrough().

    Key differences by granularity:
    - "no": PI = initial_weight, PR = resulting_weight
    - lookthrough (has underlying): PI = initial_exposure_weight_{suffix} x weight_{suffix}
    - lookthrough (leaf, no underlying): PI = initial_exposure_weight_{suffix} alone
    """
    suffix = f"{config}_{pid}"
    fcol = f"f_{suffix}"
    is_lookthrough_level_no = (granularity == "no")

    # === Step 1: Auto-compute additional_attributes from position schema ===
    if additional_attributes is None:
        pos_schema_names = positions_lf.collect_schema().names()
        standard_exclusions = {
            'identifier', 'instrument_id', 'parent_instrument_id',
            'initial_shares', 'resulting_shares', 'blocked_shares',
            'initial_weight', 'resulting_weight',
            'initial_exposure_weight', 'resulting_exposure_weight',
            'container', 'record_type', 'weight',
        }
        additional_attributes = [
            c for c in pos_schema_names
            if c not in standard_exclusions
            and not c.startswith("f_")
            and f"_{suffix}" not in c
        ]

    # === Step 2: Filter positions by perspective ===
    position_df = positions_lf.filter(pl.col(fcol).is_not_null())

    # === Step 3: Filter null-weight positions ===
    position_df = position_df.filter(
        ~(
            ((pl.col("container").cast(pl.Utf8) == "holding")
             & pl.col(f"initial_weight_{suffix}").is_null()
             & pl.col(f"resulting_weight_{suffix}").is_null())
            | ((pl.col("container").cast(pl.Utf8) == "selected_reference")
               & pl.col(f"weight_{suffix}").is_null())
            | ((pl.col("container").cast(pl.Utf8) == "contractual_reference")
               & pl.col(f"weight_{suffix}").is_null())
        )
    )

    # === Step 4: Get composites (lookthroughs) for this granularity ===
    if granularity == "no" or lookthroughs_lf is None:
        composite_df = None
    else:
        composite_df = (
            lookthroughs_lf
            .filter(pl.col(fcol).is_not_null())
            .filter(pl.col("record_type").cast(pl.Utf8) == granularity)
        )

    # === Step 5: Join positions with composites ===
    composite_schema_names = []
    if composite_df is not None:
        composite_schema_names = composite_df.collect_schema().names()
        # Select columns needed for join + weight + any additional attributes present
        composite_select_cols = [
            "instrument_id", "parent_instrument_id",
            "sub_portfolio_id", "container",
            f"weight_{suffix}",
        ]
        for attr in additional_attributes:
            if attr in composite_schema_names and attr not in composite_select_cols:
                composite_select_cols.append(attr)
        for shares_col in ["initial_shares", "resulting_shares", "blocked_shares"]:
            if shares_col in composite_schema_names and shares_col not in composite_select_cols:
                composite_select_cols.append(shares_col)

        joined_df = position_df.join(
            composite_df.select(composite_select_cols),
            left_on=["instrument_id", "container", "sub_portfolio_id"],
            right_on=["parent_instrument_id", "container", "sub_portfolio_id"],
            how="left",
            suffix="_right",
        )
    else:
        joined_df = position_df.with_columns([
            pl.lit(None, dtype=pl.Int64).alias("instrument_id_right"),
            pl.lit(None, dtype=pl.Float64).alias(f"weight_{suffix}_right"),
        ])

    # === Step 6: Calculate PI, PR, BS, BC ===
    joined_df = joined_df.with_columns([
        pl.when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_not_null())
        ).then(pl.col(f"initial_exposure_weight_{suffix}") * pl.col(f"weight_{suffix}_right"))
        .when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_null())
            & (not is_lookthrough_level_no)
        ).then(pl.col(f"initial_exposure_weight_{suffix}"))
        .when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_null())
            & is_lookthrough_level_no
        ).then(pl.col(f"initial_weight_{suffix}"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("PI"),

        pl.when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_not_null())
        ).then(pl.col(f"resulting_exposure_weight_{suffix}") * pl.col(f"weight_{suffix}_right"))
        .when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_null())
            & (not is_lookthrough_level_no)
        ).then(pl.col(f"resulting_exposure_weight_{suffix}"))
        .when(
            (pl.col("container").cast(pl.Utf8) == "holding")
            & (pl.col("instrument_id_right").is_null())
            & is_lookthrough_level_no
        ).then(pl.col(f"resulting_weight_{suffix}"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("PR"),

        pl.when(pl.col("container").cast(pl.Utf8) == "contractual_reference")
        .then(pl.col(f"weight_{suffix}"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("BC"),

        pl.when(pl.col("container").cast(pl.Utf8) == "selected_reference")
        .then(pl.col(f"weight_{suffix}"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("BS"),
    ])

    # === Step 7: Propagate additional_attributes from composites ===
    if composite_df is not None:
        joined_schema = joined_df.collect_schema().names()
        attr_exprs = []
        for col in additional_attributes:
            if col + "_right" in joined_schema:
                attr_exprs.append(
                    pl.when(pl.col("instrument_id_right").is_not_null())
                    .then(pl.col(col + "_right"))
                    .otherwise(pl.col(col))
                    .alias(col)
                )
        if attr_exprs:
            joined_df = joined_df.with_columns(attr_exprs)

    # === Step 8: Set parent_instrument_id ===
    joined_df = joined_df.with_columns([
        pl.when(pl.col("instrument_id_right").is_not_null())
        .then(pl.col("instrument_id"))
        .otherwise(pl.col("instrument_id"))
        .alias("parent_instrument_id_new"),
    ])

    # === Step 9: Update instrument_id (use underlying if exists) ===
    joined_df = joined_df.with_columns([
        pl.when(pl.col("instrument_id_right").is_not_null())
        .then(pl.col("instrument_id_right"))
        .otherwise(pl.col("instrument_id"))
        .alias("instrument_id"),
    ])
    joined_df = joined_df.with_columns([
        pl.col("parent_instrument_id_new").alias("parent_instrument_id"),
    ])

    # === Step 10: Handle shares from composites ===
    joined_schema = joined_df.collect_schema().names()
    shares_exprs = []
    for val in ["initial_shares", "resulting_shares", "blocked_shares"]:
        if val in joined_schema and val + "_right" in joined_schema:
            shares_exprs.append(
                pl.when(pl.col("instrument_id_right").is_not_null())
                .then(pl.col(val + "_right"))
                .otherwise(pl.col(val))
                .alias(val)
            )
    if shares_exprs:
        joined_df = joined_df.with_columns(shares_exprs)

    # === Step 11: Add scale_holdings as column ===
    joined_df = joined_df.with_columns(pl.lit(scale_holdings).alias("scale_holdings"))

    # === Step 12: Join with value_metadata (BEFORE grouping, matches old code) ===
    if value_metadata is not None:
        joined_df = joined_df.join(value_metadata, how="cross")
    else:
        joined_df = joined_df.with_columns([
            pl.lit(1_000_000.0).alias("total_net_assets_initial_portfolio"),
            pl.lit(1_000_000.0).alias("total_net_assets_resulting_portfolio"),
            pl.lit(1_000_000.0).alias("total_net_assets_selected_benchmark"),
            pl.lit(1_000_000.0).alias("total_net_assets_contractual_benchmark"),
        ])

    # === Step 13: Calculate PIV/PRV/BSV/BCV (BEFORE grouping, matches old code) ===
    if scale_holdings and scale_factors_lf is not None:
        # Pivot long-format scale_factors into wide-format columns:
        # (config, perspective_id, container, weight_label, scale_factor)
        # -> scale_factors_holding_initial_weight, scale_factors_selected_reference_weight, etc.
        sf = scale_factors_lf.filter(
            (pl.col("config") == config) & (pl.col("perspective_id") == pid)
        ).collect()
        sf_dict = {}
        for row in sf.iter_rows(named=True):
            key = f"scale_factors_{row['container']}_{row['weight_label']}"
            sf_dict[key] = row["scale_factor"]

        sf_cols = [
            pl.lit(sf_dict.get(k, 1.0)).alias(k)
            for k in [
                "scale_factors_holding_initial_weight",
                "scale_factors_holding_resulting_weight",
                "scale_factors_holding_initial_exposure_weight",
                "scale_factors_holding_resulting_exposure_weight",
                "scale_factors_selected_reference_weight",
                "scale_factors_contractual_reference_weight",
            ]
        ]
        joined_df = joined_df.with_columns(sf_cols)

        joined_df = joined_df.with_columns([
            pl.when(pl.col("scale_holdings"))
            .then(pl.col("PI") * pl.col("total_net_assets_initial_portfolio") * pl.col("scale_factors_holding_initial_weight"))
            .otherwise(pl.col("PI") * pl.col("total_net_assets_initial_portfolio"))
            .alias("PIV"),
            pl.when(pl.col("scale_holdings"))
            .then(pl.col("PR") * pl.col("total_net_assets_resulting_portfolio") * pl.col("scale_factors_holding_resulting_weight"))
            .otherwise(pl.col("PR") * pl.col("total_net_assets_resulting_portfolio"))
            .alias("PRV"),
            pl.when(pl.col("scale_holdings"))
            .then(pl.col("BS") * pl.col("total_net_assets_selected_benchmark") * pl.col("scale_factors_selected_reference_weight"))
            .otherwise(pl.col("BS") * pl.col("total_net_assets_selected_benchmark"))
            .alias("BSV"),
            pl.when(pl.col("scale_holdings"))
            .then(pl.col("BC") * pl.col("total_net_assets_contractual_benchmark") * pl.col("scale_factors_contractual_reference_weight"))
            .otherwise(pl.col("BC") * pl.col("total_net_assets_contractual_benchmark"))
            .alias("BCV"),
        ])
    else:
        joined_df = joined_df.with_columns([
            (pl.col("PI") * pl.col("total_net_assets_initial_portfolio")).alias("PIV"),
            (pl.col("PR") * pl.col("total_net_assets_resulting_portfolio")).alias("PRV"),
            (pl.col("BS") * pl.col("total_net_assets_selected_benchmark")).alias("BSV"),
            (pl.col("BC") * pl.col("total_net_assets_contractual_benchmark")).alias("BCV"),
        ])

    # === Step 14: Group by (matches old code structure) ===
    value_cols = ["PI", "PIV", "PR", "PRV", "BS", "BSV", "BC", "BCV"]
    shares_columns = [c for c in ["initial_shares", "resulting_shares", "blocked_shares"]
                      if c in joined_df.collect_schema().names()]
    value_cols.extend(shares_columns)

    group_by_cols = additional_attributes + [
        "instrument_id",
        "parent_instrument_id",
        "scale_holdings",
        "total_net_assets_initial_portfolio",
        "total_net_assets_resulting_portfolio",
    ]

    joined_df = joined_df.group_by(group_by_cols).agg(
        [cs.numeric().sum(), cs.exclude(cs.numeric()).first()]
    )

    # === Step 15: Rescale (always runs, filtered by scale_holdings column) ===
    rescale_cols = ["PI", "PR", "BS", "BC"]
    for m in rescale_cols:
        total_m = pl.col(m).filter(pl.col("scale_holdings")).sum()
        factor = pl.when(total_m.ne(0)).then(1.0 / total_m).otherwise(1.0)
        joined_df = joined_df.with_columns([
            (pl.col(m) * factor).alias(m),
            (pl.col(f"{m}V") * factor).alias(f"{m}V"),
        ])

    # === Step 16: Select to_keep columns ===
    to_keep = additional_attributes + value_cols + [
        "instrument_id",
        "parent_instrument_id",
        "scale_holdings",
        "total_net_assets_initial_portfolio",
        "total_net_assets_resulting_portfolio",
    ]
    joined_df = joined_df.select(to_keep)

    # === Step 17: Fill nulls ===
    joined_df = joined_df.with_columns([
        pl.col(c).fill_null(0.0).fill_nan(0.0).alias(c)
        for c in value_cols
    ])

    # === Step 18: Filter small positions ===
    joined_df = joined_df.with_columns([
        pl.when(pl.col(c).abs().le(1e-15)).then(0.0).otherwise(pl.col(c)).alias(c)
        for c in ["PI", "PR", "BS", "BC"]
    ])

    # === Step 19: Calculate active weights ===
    joined_df = joined_df.with_columns([
        (pl.col("PI") - pl.col("BC")).alias("AIC"),
        (pl.col("PR") - pl.col("BC")).alias("ARC"),
        (pl.col("PI") - pl.col("BS")).alias("AIS"),
        (pl.col("PR") - pl.col("BS")).alias("ARS"),
    ])

    # === Step 20: Calculate active values ===
    joined_df = joined_df.with_columns([
        (pl.col("total_net_assets_initial_portfolio") * pl.col("AIC")).alias("AICV"),
        (pl.col("total_net_assets_resulting_portfolio") * pl.col("ARC")).alias("ARCV"),
        (pl.col("total_net_assets_initial_portfolio") * pl.col("AIS")).alias("AISV"),
        (pl.col("total_net_assets_resulting_portfolio") * pl.col("ARS")).alias("ARSV"),
    ])

    return joined_df


def build_perspective_dict(
    positions_lf: pl.LazyFrame,
    lookthroughs_lf: Optional[pl.LazyFrame],
    scale_factors_lf: Optional[pl.LazyFrame],
    perspective_configs: Dict[str, Dict[str, List[str]]],
    value_metadata: Optional[pl.LazyFrame] = None,
    scale_holdings: bool = False,
    record_types: List[str] = None,
) -> Dict[str, Dict[int, Dict[str, pl.LazyFrame]]]:
    """
    Build dict structure: {config: {pid: {granularity: LazyFrame}}}

    All returned LazyFrames are lazy - call .collect() when you need results.
    """
    if record_types is None:
        record_types = ["essential_lookthroughs", "complete_lookthroughs"]

    result: Dict[str, Dict[int, Dict[str, pl.LazyFrame]]] = {}

    for config, pmap in perspective_configs.items():
        result[config] = {}
        for pid_str in pmap:
            pid = int(pid_str)
            result[config][pid] = {}

            # "no" granularity
            result[config][pid]["no"] = calculate_single_lookthrough(
                positions_lf=positions_lf,
                lookthroughs_lf=lookthroughs_lf,
                scale_factors_lf=scale_factors_lf,
                config=config,
                pid=pid,
                granularity="no",
                value_metadata=value_metadata,
                scale_holdings=scale_holdings,
            )

            # Lookthrough granularities
            for rt in record_types:
                result[config][pid][rt] = calculate_single_lookthrough(
                    positions_lf=positions_lf,
                    lookthroughs_lf=lookthroughs_lf,
                    scale_factors_lf=scale_factors_lf,
                    config=config,
                    pid=pid,
                    granularity=rt,
                    value_metadata=value_metadata,
                    scale_holdings=scale_holdings,
                )

    return result


# =============================================================================
# Tests
# =============================================================================


def test_no_granularity_uses_raw_weights():
    """
    "no" granularity uses initial_weight/resulting_weight directly.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.6,
            "resulting_weight": 0.5,
            "initial_exposure_weight": 0.8,  # NOT used for "no"
            "resulting_exposure_weight": 0.7,
            "weight": None,
        },
        {
            "identifier": "s1",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "selected_reference",
            "record_type": "position",
            "initial_weight": None,
            "resulting_weight": None,
            "initial_exposure_weight": None,
            "resulting_exposure_weight": None,
            "weight": 0.3,
        },
        {
            "identifier": "c1",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "contractual_reference",
            "record_type": "position",
            "initial_weight": None,
            "resulting_weight": None,
            "initial_exposure_weight": None,
            "resulting_exposure_weight": None,
            "weight": 0.25,
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight"], []),
        "selected_reference": (["weight"], []),
        "contractual_reference": (["weight"], []),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {"-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}}

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=None,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    result = calculate_single_lookthrough(
        positions_lf=raw["positions"],
        lookthroughs_lf=None,
        scale_factors_lf=None,
        config="test_cfg",
        pid=-1,
        granularity="no",
    )

    df = result.collect()

    # Should use RAW weights (0.6), NOT exposure (0.8)
    assert df.filter(pl.col("PI").is_not_null())["PI"].to_list()[0] == pytest.approx(0.6)
    assert df.filter(pl.col("PR").is_not_null())["PR"].to_list()[0] == pytest.approx(0.5)

    # Verify active weights
    pi_row = df.filter(pl.col("PI") > 0)
    assert pi_row["AIC"].to_list()[0] == pytest.approx(0.6 - 0.25)
    assert pi_row["AIS"].to_list()[0] == pytest.approx(0.6 - 0.3)


def test_lookthrough_with_underlying_uses_exposure_times_weight():
    """
    Lookthrough with underlying uses initial_exposure_weight × weight.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1",
            "instrument_id": 100,  # Parent (fund)
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.5,  # NOT used
            "resulting_weight": 0.4,
            "initial_exposure_weight": 0.6,  # USED
            "resulting_exposure_weight": 0.5,
            "weight": None,
        },
    ])

    lookthroughs_lf = pl.LazyFrame([
        {
            "identifier": "lt1",
            "instrument_id": 1,  # Underlying
            "parent_instrument_id": 100,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "essential_lookthroughs",
            "initial_weight": None,
            "resulting_weight": None,
            "initial_exposure_weight": None,
            "resulting_exposure_weight": None,
            "weight": 0.3,  # Child weight
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight", "weight"], ["weight"]),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {"-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}}

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    result = calculate_single_lookthrough(
        positions_lf=raw["positions"],
        lookthroughs_lf=raw["lookthroughs"],
        scale_factors_lf=None,
        config="test_cfg",
        pid=-1,
        granularity="essential_lookthroughs",
    )

    df = result.collect()

    # PI = initial_exposure_weight (0.6) × weight (0.3) = 0.18
    pi_row = df.filter(pl.col("PI") > 0)
    assert len(pi_row) == 1
    assert pi_row["PI"].to_list()[0] == pytest.approx(0.18)
    assert pi_row["PR"].to_list()[0] == pytest.approx(0.5 * 0.3)  # 0.15

    # instrument_id should be the underlying (1), not parent (100)
    assert pi_row["instrument_id"].to_list()[0] == 1
    # parent_instrument_id should be the parent (100)
    assert pi_row["parent_instrument_id"].to_list()[0] == 100


def test_lookthrough_leaf_uses_exposure_alone():
    """
    Lookthrough level but no underlying (leaf) uses initial_exposure_weight alone.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1",
            "instrument_id": 1,  # This position has no lookthrough children
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.5,  # NOT used for lookthrough
            "resulting_weight": 0.4,
            "initial_exposure_weight": 0.6,  # USED (alone, no multiplication)
            "resulting_exposure_weight": 0.5,
            "weight": None,
        },
    ])

    # Empty lookthroughs - no underlyings for this position
    lookthroughs_lf = pl.LazyFrame([
        {
            "identifier": "lt_other",
            "instrument_id": 999,
            "parent_instrument_id": 999,  # Different parent
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "essential_lookthroughs",
            "initial_weight": None,
            "resulting_weight": None,
            "initial_exposure_weight": None,
            "resulting_exposure_weight": None,
            "weight": 0.3,
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight", "weight"], ["weight"]),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {"-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}}

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    result = calculate_single_lookthrough(
        positions_lf=raw["positions"],
        lookthroughs_lf=raw["lookthroughs"],
        scale_factors_lf=None,
        config="test_cfg",
        pid=-1,
        granularity="essential_lookthroughs",  # Lookthrough level
    )

    df = result.collect()

    # Leaf position at lookthrough level: PI = initial_exposure_weight alone (0.6)
    pi_row = df.filter(pl.col("PI") > 0)
    assert len(pi_row) == 1
    assert pi_row["PI"].to_list()[0] == pytest.approx(0.6)
    assert pi_row["PR"].to_list()[0] == pytest.approx(0.5)

    # instrument_id and parent_instrument_id should be same for leaf
    assert pi_row["instrument_id"].to_list()[0] == 1
    assert pi_row["parent_instrument_id"].to_list()[0] == 1


def test_grouping_sums_duplicate_instruments():
    """
    When same instrument appears multiple times, weights are summed.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1a",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.3,
            "resulting_weight": 0.25,
            "initial_exposure_weight": 0.3,
            "resulting_exposure_weight": 0.25,
            "weight": None,
        },
        {
            "identifier": "h1b",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.2,
            "resulting_weight": 0.15,
            "initial_exposure_weight": 0.2,
            "resulting_exposure_weight": 0.15,
            "weight": None,
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight", "weight"], []),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {"-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}}

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=None,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    result = calculate_single_lookthrough(
        positions_lf=raw["positions"],
        lookthroughs_lf=None,
        scale_factors_lf=None,
        config="test_cfg",
        pid=-1,
        granularity="no",
    )

    df = result.collect()

    # Should be grouped: PI = 0.3 + 0.2 = 0.5
    pi_row = df.filter(pl.col("PI") > 0)
    assert len(pi_row) == 1
    assert pi_row["PI"].to_list()[0] == pytest.approx(0.5)
    assert pi_row["PR"].to_list()[0] == pytest.approx(0.4)


def test_small_positions_filtered():
    """
    Weights <= 1e-15 are set to 0.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1",
            "instrument_id": 1,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 1e-16,  # Too small
            "resulting_weight": 0.5,
            "initial_exposure_weight": 1e-16,
            "resulting_exposure_weight": 0.5,
            "weight": None,
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight", "weight"], []),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {"-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}}

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=None,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    result = calculate_single_lookthrough(
        positions_lf=raw["positions"],
        lookthroughs_lf=None,
        scale_factors_lf=None,
        config="test_cfg",
        pid=-1,
        granularity="no",
    )

    df = result.collect()

    # Small weight should be filtered to 0
    pi_row = df.filter(pl.col("instrument_id") == 1)
    assert pi_row["PI"].to_list()[0] == pytest.approx(0.0)
    assert pi_row["PR"].to_list()[0] == pytest.approx(0.5)


def test_build_perspective_dict_structure():
    """
    Build complete dict structure for all perspectives and granularities.
    """
    positions_lf = pl.LazyFrame([
        {
            "identifier": "h1",
            "instrument_id": 100,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "position",
            "initial_weight": 0.6,
            "resulting_weight": 0.5,
            "initial_exposure_weight": 0.7,
            "resulting_exposure_weight": 0.6,
            "weight": None,
        },
    ])

    lookthroughs_lf = pl.LazyFrame([
        {
            "identifier": "lt1",
            "instrument_id": 1,
            "parent_instrument_id": 100,
            "sub_portfolio_id": 10,
            "container": "holding",
            "record_type": "essential_lookthroughs",
            "initial_weight": None,
            "resulting_weight": None,
            "initial_exposure_weight": None,
            "resulting_exposure_weight": None,
            "weight": 0.5,
        },
    ])

    weight_labels_map = {
        "holding": (["initial_weight", "resulting_weight", "initial_exposure_weight", "resulting_exposure_weight", "weight"], ["weight"]),
    }

    perspective_configs = {"cfg_a": {"-1": [], "-2": []}}
    custom_perspective_rules = {
        "-1": {"rules": [{"criteria": {}, "apply_to": "both"}]},
        "-2": {"rules": [{"criteria": {}, "apply_to": "both"}]},
    }

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []
    raw = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    frames = build_perspective_dict(
        positions_lf=raw["positions"],
        lookthroughs_lf=raw["lookthroughs"],
        scale_factors_lf=raw.get("scale_factors"),
        perspective_configs=perspective_configs,
        record_types=["essential_lookthroughs"],
    )

    # Verify structure
    assert "cfg_a" in frames
    assert -1 in frames["cfg_a"]
    assert -2 in frames["cfg_a"]
    assert "no" in frames["cfg_a"][-1]
    assert "essential_lookthroughs" in frames["cfg_a"][-1]

    # "no" uses raw weight (0.6)
    df_no = frames["cfg_a"][-1]["no"].collect()
    pi_no = df_no.filter(pl.col("PI") > 0)["PI"].to_list()[0]
    assert pi_no == pytest.approx(0.6)

    # lookthrough uses exposure × weight (0.7 × 0.5 = 0.35)
    df_lt = frames["cfg_a"][-1]["essential_lookthroughs"].collect()
    pi_lt = df_lt.filter(pl.col("PI") > 0)["PI"].to_list()[0]
    assert pi_lt == pytest.approx(0.35)


# =============================================================================
# Old downstream pipeline: dict output → DataFrames → PositionEngine
# Replicates _parse_perspective_collections + DataPreperation + PositionEngine
# =============================================================================

CONTAINER_TO_CATEGORY = {
    "holding": "holdings",
    "selected_reference": "selected",
    "contractual_reference": "contractual",
}

LT_TYPE_TO_GRANULARITY = {
    "essential_lookthroughs": "essential",
    "complete_lookthroughs": "complete",
}


def psp_dict_to_positions_df(
    psp_result: Dict,
    input_positions_lf: pl.LazyFrame,
) -> pl.LazyFrame:
    """Convert PSP dict output positions into old downstream DataFrame format.

    Replicates: _parse_perspective_collections → parse_instrument_collections
    → DataPreperation.prepare_positions
    """
    input_df = input_positions_lf.collect()

    rows = []
    configs = psp_result["perspective_configurations"]
    for config_name, perspectives in configs.items():
        for pid, containers in perspectives.items():
            for container, collections in containers.items():
                category = CONTAINER_TO_CATEGORY.get(container)
                if category is None or "positions" not in collections:
                    continue

                for identifier, weights in collections["positions"].items():
                    # Find original position by identifier and container
                    orig_rows = input_df.filter(
                        (pl.col("identifier") == identifier)
                        & (pl.col("container") == container)
                    )
                    if orig_rows.is_empty():
                        continue

                    row = orig_rows.to_dicts()[0]
                    # Drop PSP-internal columns
                    for col_name in ["identifier", "container", "record_type"]:
                        row.pop(col_name, None)

                    # Override weight values with PSP output (perspective-weighted)
                    # Clear existing weight columns first (PSP output has the correct values)
                    for w in ["initial_weight", "resulting_weight",
                              "initial_exposure_weight", "resulting_exposure_weight", "weight"]:
                        if w not in weights:
                            row.pop(w, None)
                    row.update(weights)

                    # Add downstream columns
                    row["perspective"] = pid
                    row["category"] = category

                    rows.append(row)

    if not rows:
        return pl.DataFrame().lazy()
    return pl.DataFrame(rows, infer_schema_length=None).lazy()


def psp_dict_to_composites_df(
    psp_result: Dict,
    input_lt_lf: Optional[pl.LazyFrame],
) -> pl.LazyFrame:
    """Convert PSP dict output lookthroughs into old downstream composites DataFrame.

    Replicates: _parse_perspective_collections → create_composites_from_underlyings
    → DataPreperation.prepare_composites → process_composite_list
    """
    if input_lt_lf is None:
        return pl.DataFrame({"composite_id": [], "instrument_id": [],
                             "sub_portfolio_id": [], "perspective": [],
                             "category": [], "granularity": [], "weight": []}).lazy()

    input_df = input_lt_lf.collect()

    rows = []
    configs = psp_result["perspective_configurations"]
    for config_name, perspectives in configs.items():
        for pid, containers in perspectives.items():
            for container, collections in containers.items():
                category = CONTAINER_TO_CATEGORY.get(container)
                if category is None:
                    continue

                for lt_type, granularity in LT_TYPE_TO_GRANULARITY.items():
                    if lt_type not in collections:
                        continue

                    for identifier, weights in collections[lt_type].items():
                        # Find original lookthrough
                        orig_rows = input_df.filter(
                            (pl.col("identifier") == identifier)
                            & (pl.col("container") == container)
                        )
                        if orig_rows.is_empty():
                            continue

                        orig = orig_rows.to_dicts()[0]
                        row = {
                            "composite_id": orig["parent_instrument_id"],
                            "instrument_id": orig["instrument_id"],
                            "sub_portfolio_id": orig.get("sub_portfolio_id", 0),
                            "perspective": pid,
                            "category": category,
                            "granularity": granularity,
                        }
                        row.update(weights)
                        rows.append(row)

    if not rows:
        return pl.DataFrame({"composite_id": [], "instrument_id": [],
                             "sub_portfolio_id": [], "perspective": [],
                             "category": [], "granularity": [], "weight": []}).lazy()
    return pl.DataFrame(rows, infer_schema_length=None).lazy()


def psp_dict_to_metadata_df(
    psp_result: Dict,
    scale_holdings: bool = False,
    tna: float = 1_000_000.0,
) -> pl.LazyFrame:
    """Create metadata DataFrame for old downstream.

    Replicates: DataPreperation.prepare_metadata + build_metadata_dictionary
    """
    rows = []
    configs = psp_result["perspective_configurations"]
    for config_name, perspectives in configs.items():
        for pid, containers in perspectives.items():
            sf_holding = containers.get("holding", {}).get("scale_factors", {})
            sf_selected = containers.get("selected_reference", {}).get("scale_factors", {})
            sf_contractual = containers.get("contractual_reference", {}).get("scale_factors", {})

            row = {
                "perspective": pid,
                "scale_holdings": scale_holdings,
                "total_net_assets_initial_portfolio": tna,
                "total_net_assets_resulting_portfolio": tna,
                "total_net_assets_selected_benchmark": tna,
                "total_net_assets_contractual_benchmark": tna,
                "scale_factors_holding_initial_weight": sf_holding.get("initial_weight", 1.0),
                "scale_factors_holding_resulting_weight": sf_holding.get("resulting_weight", 1.0),
                "scale_factors_holding_initial_exposure_weight": sf_holding.get("initial_exposure_weight", 1.0),
                "scale_factors_holding_resulting_exposure_weight": sf_holding.get("resulting_exposure_weight", 1.0),
                "scale_factors_selected_reference_weight": sf_selected.get("weight", 1.0),
                "scale_factors_contractual_reference_weight": sf_contractual.get("weight", 1.0),
            }
            rows.append(row)

    return pl.DataFrame(rows).lazy()


def old_calculate_single_lookthrough(
    position_df: pl.LazyFrame,
    composite_df: pl.LazyFrame,
    granularity: str,
    value_metadata: pl.LazyFrame,
) -> pl.LazyFrame:
    """Old PositionEngine.calculate_single_lookthrough — direct port.

    Uses category/perspective/composite_id/granularity (old column naming).
    Input DataFrames come from psp_dict_to_*_df() conversion helpers.
    """
    composite_df = composite_df.with_columns(pl.col("sub_portfolio_id").fill_null(0))
    position_df = position_df.with_columns(pl.col("sub_portfolio_id").fill_null(0))

    # Filter null-weight positions (matches old _clean_collections)
    position_df = position_df.filter(
        ~(
            ((pl.col("category") == "holdings")
             & pl.col("initial_weight").is_null()
             & pl.col("resulting_weight").is_null())
            | ((pl.col("category") == "selected") & pl.col("weight").is_null())
            | ((pl.col("category") == "contractual") & pl.col("weight").is_null())
        )
    )

    filtered_composite_df = composite_df.filter(pl.col("granularity") == granularity)
    is_lookthrough_level_no = granularity == "no"

    position_columns = position_df.collect_schema().names()
    composite_columns = composite_df.collect_schema().names()

    additional_attributes = list(
        set(position_columns) - {
            'initial_shares', 'resulting_shares', 'blocked_shares',
            'initial_weight', 'resulting_weight',
            'initial_exposure_weight', 'resulting_exposure_weight',
            'perspective', 'category', 'granularity',
            'parent_instrument_id', 'instrument_id', 'weight',
        }
    )

    # Join positions with composites
    joined_df = position_df.join(
        filtered_composite_df,
        how="left",
        left_on=["instrument_id", "perspective", "category", "sub_portfolio_id"],
        right_on=["composite_id", "perspective", "category", "sub_portfolio_id"],
        coalesce=False,
    )

    # PI/PR/BC/BS
    joined_df = joined_df.with_columns([
        pl.when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_not_null())
        .then(pl.col("initial_exposure_weight") * pl.col("weight_right"))
        .when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_null() & (not is_lookthrough_level_no))
        .then(pl.col("initial_exposure_weight"))
        .when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_null() & is_lookthrough_level_no)
        .then(pl.col("initial_weight"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("PI"),

        pl.when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_not_null())
        .then(pl.col("resulting_exposure_weight") * pl.col("weight_right"))
        .when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_null() & (not is_lookthrough_level_no))
        .then(pl.col("resulting_exposure_weight"))
        .when((pl.col("category") == "holdings") & pl.col("instrument_id_right").is_null() & is_lookthrough_level_no)
        .then(pl.col("resulting_weight"))
        .otherwise(pl.lit(None, dtype=pl.Float64))
        .alias("PR"),

        pl.when(pl.col("category") == "contractual")
        .then(pl.col("weight")).otherwise(pl.lit(None, dtype=pl.Float64)).alias("BC"),

        pl.when(pl.col("category") == "selected")
        .then(pl.col("weight")).otherwise(pl.lit(None, dtype=pl.Float64)).alias("BS"),
    ])

    # Propagate additional attributes from composites
    joined_schema = joined_df.collect_schema().names()
    attr_exprs = []
    for col in additional_attributes:
        if col + "_right" in joined_schema:
            attr_exprs.append(
                pl.when(pl.col("instrument_id_right").is_not_null())
                .then(pl.col(col + "_right")).otherwise(pl.col(col)).alias(col)
            )
    if attr_exprs:
        joined_df = joined_df.with_columns(attr_exprs)

    # Set parent_instrument_id from composite_id or self
    joined_df = joined_df.with_columns([
        pl.when(pl.col("instrument_id_right").is_not_null())
        .then(pl.col("composite_id"))
        .otherwise(pl.col("instrument_id"))
        .alias("parent_instrument_id")
    ])

    # Update instrument_id to underlying if exists
    joined_df = joined_df.with_columns([
        pl.when(pl.col("instrument_id_right").is_not_null())
        .then(pl.col("instrument_id_right"))
        .otherwise(pl.col("instrument_id"))
        .alias("instrument_id")
    ])

    # Join value_metadata on perspective (BEFORE grouping)
    joined_df = joined_df.join(value_metadata, how="inner", on="perspective")

    scale_holdings_col = pl.col("scale_holdings")

    # PIV/PRV/BSV/BCV (BEFORE grouping)
    joined_df = joined_df.with_columns([
        pl.when(scale_holdings_col)
        .then(pl.col("PI") * pl.col("total_net_assets_initial_portfolio") * pl.col("scale_factors_holding_initial_weight"))
        .otherwise(pl.col("PI") * pl.col("total_net_assets_initial_portfolio"))
        .alias("PIV"),
        pl.when(scale_holdings_col)
        .then(pl.col("PR") * pl.col("total_net_assets_resulting_portfolio") * pl.col("scale_factors_holding_resulting_weight"))
        .otherwise(pl.col("PR") * pl.col("total_net_assets_resulting_portfolio"))
        .alias("PRV"),
        pl.when(scale_holdings_col)
        .then(pl.col("BC") * pl.col("total_net_assets_contractual_benchmark") * pl.col("scale_factors_contractual_reference_weight"))
        .otherwise(pl.col("BC") * pl.col("total_net_assets_contractual_benchmark"))
        .alias("BCV"),
        pl.when(scale_holdings_col)
        .then(pl.col("BS") * pl.col("total_net_assets_selected_benchmark") * pl.col("scale_factors_selected_reference_weight"))
        .otherwise(pl.col("BS") * pl.col("total_net_assets_selected_benchmark"))
        .alias("BSV"),
    ])

    # Group by
    value_cols = ["PI", "PIV", "PR", "PRV", "BS", "BSV", "BC", "BCV"]
    group_by_cols = additional_attributes + [
        "perspective", "instrument_id", "parent_instrument_id",
        "scale_holdings",
        "total_net_assets_initial_portfolio", "total_net_assets_resulting_portfolio",
    ]

    joined_df = joined_df.group_by(group_by_cols).agg(
        [cs.numeric().sum(), cs.exclude(cs.numeric()).first()]
    )

    # Rescale (always runs, filtered by scale_holdings)
    rescale_expr = []
    for m in ["PI", "PR", "BS", "BC"]:
        total_m = pl.col(m).filter(pl.col("scale_holdings")).sum().over("perspective")
        factor = pl.when(total_m.ne(0)).then(1.0 / total_m).otherwise(1.0)
        rescale_expr.extend([
            (pl.col(m) * factor).alias(m),
            (pl.col(f"{m}V") * factor).alias(f"{m}V"),
        ])
    joined_df = joined_df.with_columns(rescale_expr)

    # Select to_keep
    to_keep = additional_attributes + value_cols + [
        "perspective", "instrument_id", "parent_instrument_id",
        "scale_holdings",
        "total_net_assets_initial_portfolio", "total_net_assets_resulting_portfolio",
    ]
    joined_df = joined_df.select(to_keep)

    joined_df = joined_df.with_columns(
        pl.col(value_cols).fill_null(0).fill_nan(0)
    )

    joined_df = joined_df.with_columns([
        pl.when(pl.col(c).abs().le(1e-15)).then(0.0).otherwise(pl.col(c)).alias(c)
        for c in ["PI", "PR", "BS", "BC"]
    ])

    joined_df = joined_df.with_columns([
        (pl.col("PI") - pl.col("BC")).alias("AIC"),
        (pl.col("PR") - pl.col("BC")).alias("ARC"),
        (pl.col("PI") - pl.col("BS")).alias("AIS"),
        (pl.col("PR") - pl.col("BS")).alias("ARS"),
    ])

    joined_df = joined_df.with_columns([
        (pl.col("total_net_assets_initial_portfolio") * pl.col("AIC")).alias("AICV"),
        (pl.col("total_net_assets_resulting_portfolio") * pl.col("ARC")).alias("ARCV"),
        (pl.col("total_net_assets_initial_portfolio") * pl.col("AIS")).alias("AISV"),
        (pl.col("total_net_assets_resulting_portfolio") * pl.col("ARS")).alias("ARSV"),
    ])

    return joined_df


# =============================================================================
# End-to-end comparison test: dict path vs raw DF path
# =============================================================================


def test_dict_vs_raw_equivalence():
    """Prove dict output path (old downstream) and raw DF path produce identical results."""

    # --- Test data: all three containers + lookthrough ---
    positions_lf = pl.LazyFrame([
        {
            "identifier": 100, "instrument_id": 100, "sub_portfolio_id": 10,
            "container": "holding", "record_type": "positions",
            "initial_weight": 0.6, "resulting_weight": 0.5,
            "initial_exposure_weight": 0.7, "resulting_exposure_weight": 0.6,
            "weight": None,
        },
        {
            "identifier": 200, "instrument_id": 200, "sub_portfolio_id": 10,
            "container": "selected_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.3,
        },
        {
            "identifier": 300, "instrument_id": 300, "sub_portfolio_id": 10,
            "container": "contractual_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.25,
        },
    ])

    lookthroughs_lf = pl.LazyFrame([
        {
            "identifier": 1, "instrument_id": 1, "parent_instrument_id": 100,
            "sub_portfolio_id": 10, "container": "holding",
            "record_type": "essential_lookthroughs",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.4,
        },
    ])

    weight_labels_map = {
        "holding": (
            ["initial_weight", "resulting_weight", "initial_exposure_weight",
             "resulting_exposure_weight", "weight"],
            ["weight"],
        ),
        "selected_reference": (["weight"], []),
        "contractual_reference": (["weight"], []),
    }

    perspective_configs = {"test_cfg": {"-1": []}}
    custom_perspective_rules = {
        "-1": {"rules": [{"criteria": {}, "apply_to": "both"}]}
    }

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []

    # === Run PSP: dict output ===
    dict_result = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=False,
    )

    # === Run PSP: raw DF output ===
    raw_result = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    # === Dict output path (old downstream) ===
    pos_df = psp_dict_to_positions_df(dict_result, positions_lf)
    comp_df = psp_dict_to_composites_df(dict_result, lookthroughs_lf)
    meta_df = psp_dict_to_metadata_df(dict_result, scale_holdings=False)

    old_no = old_calculate_single_lookthrough(pos_df, comp_df, "no", meta_df).collect()
    old_lt = old_calculate_single_lookthrough(pos_df, comp_df, "essential", meta_df).collect()

    # === Raw DF path (our new downstream) ===
    new_no = calculate_single_lookthrough(
        raw_result["positions"], raw_result["lookthroughs"], None,
        "test_cfg", -1, "no",
    ).collect()
    new_lt = calculate_single_lookthrough(
        raw_result["positions"], raw_result["lookthroughs"], None,
        "test_cfg", -1, "essential_lookthroughs",
    ).collect()

    # === Compare "no" granularity ===
    # Sort both by instrument_id for stable comparison
    old_no_sorted = old_no.sort("instrument_id")
    new_no_sorted = new_no.sort("instrument_id")

    for col in ["PI", "PR", "BC", "BS", "PIV", "PRV", "BCV", "BSV", "AIC", "ARC", "AIS", "ARS"]:
        old_vals = old_no_sorted[col].to_list()
        new_vals = new_no_sorted[col].to_list()
        assert len(old_vals) == len(new_vals), f"Row count mismatch for {col} at 'no' granularity"
        for i, (o, n) in enumerate(zip(old_vals, new_vals)):
            assert o == pytest.approx(n, abs=1e-10), (
                f"Mismatch at 'no' granularity, col={col}, row={i}: old={o} vs new={n}"
            )

    # === Compare "essential" lookthrough granularity ===
    old_lt_sorted = old_lt.sort("instrument_id")
    new_lt_sorted = new_lt.sort("instrument_id")

    for col in ["PI", "PR", "BC", "BS", "PIV", "PRV", "BCV", "BSV"]:
        old_vals = old_lt_sorted[col].to_list()
        new_vals = new_lt_sorted[col].to_list()
        assert len(old_vals) == len(new_vals), f"Row count mismatch for {col} at 'essential' granularity"
        for i, (o, n) in enumerate(zip(old_vals, new_vals)):
            assert o == pytest.approx(n, abs=1e-10), (
                f"Mismatch at 'essential' granularity, col={col}, row={i}: old={o} vs new={n}"
            )


def _compare_old_vs_new(old_df, new_df, granularity_label, cols):
    """Helper: compare old vs new DataFrames on given columns."""
    old_sorted = old_df.sort("instrument_id")
    new_sorted = new_df.sort("instrument_id")

    assert old_sorted.height == new_sorted.height, (
        f"Row count mismatch at '{granularity_label}': old={old_sorted.height} vs new={new_sorted.height}"
    )

    for col in cols:
        old_vals = old_sorted[col].to_list()
        new_vals = new_sorted[col].to_list()
        for i, (o, n) in enumerate(zip(old_vals, new_vals)):
            assert o == pytest.approx(n, abs=1e-10), (
                f"Mismatch at '{granularity_label}', col={col}, row={i}: old={o} vs new={n}"
            )


def test_dict_vs_raw_filtering_rescale_complete_lt():
    """Dict vs raw equivalence with filtering rules, exposure-weighted lookthroughs,
    scale_holdings_to_100_percent, and complete_lookthroughs granularity."""

    # Holdings: 4 positions, rule will filter out instrument_id <= 100
    # So h1 (id=100) is removed, h2 (id=200), h3 (id=300), h4 (id=400) kept
    positions_lf = pl.LazyFrame([
        {
            "identifier": 100, "instrument_id": 100, "sub_portfolio_id": 10,
            "container": "holding", "record_type": "positions",
            "initial_weight": 0.15, "resulting_weight": 0.12,
            "initial_exposure_weight": 0.18, "resulting_exposure_weight": 0.14,
            "weight": None,
        },
        {
            "identifier": 200, "instrument_id": 200, "sub_portfolio_id": 10,
            "container": "holding", "record_type": "positions",
            "initial_weight": 0.30, "resulting_weight": 0.25,
            "initial_exposure_weight": 0.35, "resulting_exposure_weight": 0.28,
            "weight": None,
        },
        {
            "identifier": 300, "instrument_id": 300, "sub_portfolio_id": 10,
            "container": "holding", "record_type": "positions",
            "initial_weight": 0.25, "resulting_weight": 0.20,
            "initial_exposure_weight": 0.27, "resulting_exposure_weight": 0.22,
            "weight": None,
        },
        {
            "identifier": 400, "instrument_id": 400, "sub_portfolio_id": 10,
            "container": "holding", "record_type": "positions",
            "initial_weight": 0.30, "resulting_weight": 0.43,
            "initial_exposure_weight": 0.20, "resulting_exposure_weight": 0.36,
            "weight": None,
        },
        # Selected reference: same instruments as holdings
        {
            "identifier": 1200, "instrument_id": 200, "sub_portfolio_id": 10,
            "container": "selected_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.40,
        },
        {
            "identifier": 1300, "instrument_id": 300, "sub_portfolio_id": 10,
            "container": "selected_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.35,
        },
        {
            "identifier": 1400, "instrument_id": 400, "sub_portfolio_id": 10,
            "container": "selected_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.25,
        },
        # Contractual reference
        {
            "identifier": 2200, "instrument_id": 200, "sub_portfolio_id": 10,
            "container": "contractual_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.50,
        },
        {
            "identifier": 2300, "instrument_id": 300, "sub_portfolio_id": 10,
            "container": "contractual_reference", "record_type": "positions",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.50,
        },
    ])

    # Complete lookthroughs with exposure weights for instrument 200 (holding)
    lookthroughs_lf = pl.LazyFrame([
        {
            "identifier": 5001, "instrument_id": 5001, "parent_instrument_id": 200,
            "sub_portfolio_id": 10, "container": "holding",
            "record_type": "complete_lookthroughs",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.6,
        },
        {
            "identifier": 5002, "instrument_id": 5002, "parent_instrument_id": 200,
            "sub_portfolio_id": 10, "container": "holding",
            "record_type": "complete_lookthroughs",
            "initial_weight": None, "resulting_weight": None,
            "initial_exposure_weight": None, "resulting_exposure_weight": None,
            "weight": 0.4,
        },
    ])

    weight_labels_map = {
        "holding": (
            ["initial_weight", "resulting_weight", "initial_exposure_weight",
             "resulting_exposure_weight", "weight"],
            ["weight"],
        ),
        "selected_reference": (["weight"], []),
        "contractual_reference": (["weight"], []),
    }

    # Filtering rule: instrument_id > 100 (removes h1)
    # With scale_holdings_to_100_percent modifier
    perspective_configs = {"cfg_a": {"-1": ["scale_holdings_to_100_percent"]}}
    custom_perspective_rules = {
        "-1": {
            "rules": [{
                "apply_to": "both",
                "criteria": {
                    "column": "instrument_id",
                    "operator_type": ">",
                    "value": 100,
                },
            }]
        }
    }

    engine = PerspectiveEngine(connection_string=None)
    engine.config.default_modifiers = []

    # === Run PSP: dict output ===
    dict_result = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=False,
    )

    # === Run PSP: raw DF output ===
    raw_result = engine.process_dataframes(
        positions_lf=positions_lf,
        lookthroughs_lf=lookthroughs_lf,
        weight_labels_map=weight_labels_map,
        perspective_configs=perspective_configs,
        custom_perspective_rules=custom_perspective_rules,
        return_raw_dataframes=True,
    )

    # === Dict output path (old downstream) ===
    pos_df = psp_dict_to_positions_df(dict_result, positions_lf)
    comp_df = psp_dict_to_composites_df(dict_result, lookthroughs_lf)
    meta_df = psp_dict_to_metadata_df(dict_result, scale_holdings=True)

    old_no = old_calculate_single_lookthrough(pos_df, comp_df, "no", meta_df).collect()
    old_lt = old_calculate_single_lookthrough(pos_df, comp_df, "complete", meta_df).collect()

    # === Raw DF path (our new downstream) ===
    new_no = calculate_single_lookthrough(
        raw_result["positions"], raw_result["lookthroughs"],
        raw_result["scale_factors"],
        "cfg_a", -1, "no",
        scale_holdings=True,
    ).collect()
    new_lt = calculate_single_lookthrough(
        raw_result["positions"], raw_result["lookthroughs"],
        raw_result["scale_factors"],
        "cfg_a", -1, "complete_lookthroughs",
        scale_holdings=True,
    ).collect()

    all_cols = ["PI", "PR", "BC", "BS", "PIV", "PRV", "BCV", "BSV", "AIC", "ARC", "AIS", "ARS"]

    _compare_old_vs_new(old_no, new_no, "no", all_cols)
    _compare_old_vs_new(old_lt, new_lt, "complete", all_cols)

    # === Sanity checks: verify filtering actually happened ===
    # instrument_id 100 should have been filtered out
    assert 100 not in new_no["instrument_id"].to_list(), "instrument 100 should be filtered"

    # === Sanity checks: verify rescaling happened (weights should sum to ~1.0) ===
    holdings_pi = new_no.filter(pl.col("PI") != 0.0)["PI"].to_list()
    assert sum(holdings_pi) == pytest.approx(1.0, abs=1e-10), (
        f"After rescale, holdings PI should sum to 1.0, got {sum(holdings_pi)}"
    )
